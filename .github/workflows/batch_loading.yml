name: Process XML to JSON and HTML For Full Data Set in Batches

on:
  push:
    branches:
      - 'gaddel_development'  # current data branch
    paths:
      - '.github/workflows/batch_loading.yml'  # Trigger only on changes to this file 
      
permissions:
  id-token: write
  contents: read

jobs:
  process_and_transform:
    runs-on: ubuntu-latest
    steps:
    
      # Step 1: Checkout repositories
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Checkout Gaddel repository (production code repo)
        uses: actions/checkout@v3
        with:
          repository: srophe/Gaddel
          ref: main
          path: syriaca

      # Step 2: Install Java and Saxon for XSLT
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Check cache for Saxon JAR
        id: cache-saxon
        uses: actions/cache@v3
        with:
          path: saxon.jar
          key: saxon-10.6

      - name: Download Saxon if not cached
        if: steps.cache-saxon.outputs.cache-hit != 'true'
        run: wget https://repo1.maven.org/maven2/net/sf/saxon/Saxon-HE/10.6/Saxon-HE-10.6.jar -O saxon.jar

      # Step 3: Identify XML files for processing
      - name: Identify XML files
        run: |
          # find ./data/works/tei -name '*.xml' | head -n 5000 > xml_files.txt
          # find ./data/places/tei -name '*.xml' > xml_files.txt
          # find ./data/persons/tei -name '*.xml' | tail -n +5 > xml_files.txt
          # find ./data/bibl/tei -name '*.xml' | tail -n +25000 | head -n 5000 > xml_files.txt
          # find ./data/bibl/tei -name '*.xml' | tail -n +20000 | head -n 5000 > xml_files.txt
          # find ./data/bibl/tei -name '*.xml' | tail -n +15000 | head -n 5000 > xml_files.txt
          # find ./data/bibl/tei -name '*.xml' | tail -n +10000 | head -n 5000 > xml_files.txt
          # find ./data/bibl/tei -name '*.xml' | tail -n +5000 | head -n 5000 > xml_files.txt
          find ./data/bibl/tei -name '*.xml' | head -n 5000 > xml_files.txt

          # find ./data/taxonomy -name '*.rdf' > xml_files.txt

          echo "Processing $(wc -l < xml_files.txt) XML files"
          
      # # Step 4: Run XSLT Transformations in Parallel for JSON
      # - name: Run XSLT Transformations and Create Bulk JSON (Parallelized)
      #   run: |
      #     if [ ! -s xml_files.txt ]; then
      #       echo "No XML files to process."
      #       exit 0
      #     fi
      
      #     set +e  # Allow errors without stopping execution
      #     mkdir -p logs json_output  # Create log and JSON directories
      #     > bulk_data.json  # Clear existing bulk JSON file
      
      #     # Process each XML file in parallel
      #     cat xml_files.txt | xargs -P $(nproc) -I {} sh -c '
      #       file="$1"
      #       filename=$(basename "${file%.xml}")
      #       type=$(echo "$file" | grep -o -E "taxonomy|work|subject|person|place|bibl" | tail -n 1)
      
      #       # Fix bible/subject conflict using POSIX-compliant `test`
      #       if [ "$type" = "subject" ]; then
      #         type="subject"
      #       elif [ "$type" = "bibl" ]; then
      #         type="cbss"
      #       fi
      
      #       echo "Processing $filename for $type JSON"
      
      #       # Write index metadata to a separate temp file
      #       echo "{\"index\":{\"_index\":\"syriaca-index-12\",\"_id\":\"$type-$filename\"}}" > json_output/$filename.json
      
      #       # Run transformation, ensuring single-line output
      #       if ! java -jar saxon.jar -s:"$file" -xsl:json-stylesheet.xsl docType="$type" | tr -d "\n" >> json_output/$filename.json; then
      #         echo "::warning:: JSON transformation failed for $file"
      #         echo "Skipping $type $file due to transformation error." >> logs/errors.log
      #       fi
      
      #       echo "" >> json_output/$filename.json  # Ensure newline between entries
      #     ' sh {}
      
      #     # Merge all JSON files into a single bulk file
      #     cat json_output/*.json > bulk_data.json
      
      #     set -e  # Re-enable stopping on error

      # # Step 5: Configure AWS credentials
      # - name: Configure AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v2
      #   with:
      #     role-to-assume: ${{ secrets.AWS_SROPHE_ROLE }}
      #     aws-region: us-east-1
      #     role-session-name: GitHub-OIDC-data    

      # # Step 6: Upload JSON file to S3
      # - name: Upload JSON to S3
      #   run: |
      #     TIMESTAMP=$(date +%Y%m%d%H%M%S)
      #     aws s3 cp bulk_data.json s3://srophe-syriaca-front-end/json-data/advancedsearchfields/index_12_$TIMESTAMP.json
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

      # # Step 7: Convert XML to HTML in Parallel
      - name: Convert XML to HTML in Parallel
        run: |
          mkdir -p logs data-html
          
          cat xml_files.txt | xargs -P $(nproc) -I {} sh -c '
            file="$1"
            filename=$(basename ${file%.xml})
            type=$(echo "$file" | grep -o -E "taxonomy|work|subject|person|place|bibl" | tail -n 1)
            [ -z "$type" ] && type="unknown"
            mkdir -p data-html/$type
            
            echo "Processing $filename for HTML"
            if ! java -jar saxon.jar -s:$file -xsl:html-stylesheet.xsl -o:data-html/${type}/${filename}.html 2>> logs/errors.log; then
              echo "::warning:: XSLT transformation failed for $file. Check logs/errors.log"
              echo "Error processing $type file: $file" >> logs/errors.log
            fi
          ' sh {}
          

      # Step 8: Upload HTML files to S3 
      # For multi-hour runs, it is necessary refresh credentials, as the eariler credentials will expire
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_SROPHE_ROLE }}
          aws-region: us-east-1
          role-session-name: GitHub-OIDC-data
          
      # For faster speed and deletion of files so the data repo matches, switch to "sync" with "--delete" 
      - name: Upload HTML files to S3
        run: |
          for html_file in $(find ./data-html -name "*.html"); do
            type=$(echo "$html_file" | grep -o -E 'taxonomy|work|subject|person|place|bibl' | tail -n 1)
            echo "html_file $html_file"
            if [ "$type" == "subject" ]; then
              type="taxonomy"
            fi
            if [ "$type" == "bibl" ]; then
              type="cbss"
            fi
            # Copy html file to S3 with the idno path
            echo "uploading to $html_file s3://srophe-syriaca-front-end/${type}/$(basename ${html_file%.html})"
            aws s3 cp $html_file s3://srophe-syriaca-front-end/${type}/$(basename ${html_file%.html})
          done
          
          # Sync only the persons folder and delete obsolete files, still adding extra folder and .html
            # if [ -d "./data-html/person" ]; then
            #   echo "Syncing persons folder to S3 and deleting obsolete files..."
            #   aws s3 sync ./data-html/person s3://srophe-syriaca-front-end/person --delete
            # else
            #   echo "No persons folder found, skipping sync."
            # fi     

        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          AWS_REGION: ${{ secrets.AWS_REGION }}

      # Step 9: Upload error logs as an artifact for debugging
      - name: Upload Logs to GitHub Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: error-logs
          path: logs/errors.log
