name: Process XML to JSON and/or HTML For Full Data Set in Batches

on:
  push:
    branches:
      - 'gaddel_development'  # current data branch
    paths:
      - '.github/workflows/batch_loading.yml'  # Trigger action only on changes to this file 
      
permissions:
  id-token: write
  contents: read

jobs:
  process_and_transform:
    runs-on: ubuntu-latest
    steps:
    
      # Step 1: Checkout repositories
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Checkout Gaddel repository (production code repo)
        uses: actions/checkout@v3
        with:
          repository: srophe/Gaddel
          ref: main
          path: syriaca

      # Step 2: Install Java and Saxon for XSLT
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Check cache for Saxon JAR
        id: cache-saxon
        uses: actions/cache@v3
        with:
          path: saxon.jar
          key: saxon-10.6

      - name: Download Saxon if not cached
        if: steps.cache-saxon.outputs.cache-hit != 'true'
        run: wget https://repo1.maven.org/maven2/net/sf/saxon/Saxon-HE/10.6/Saxon-HE-10.6.jar -O saxon.jar


      # Step 3: Identify XML files for processing: Decide which subset of the data you want to process
      - name: Identify XML files
        run: |
          # Find each section of data for processing one at a time, or target files
          # find ./data/works/tei -name '*.xml' | head -n 5000 > xml_files.txt
          # find ./data/places/tei -name '*.xml' > xml_files.txt
          find ./data/persons/tei -name '*.xml' > xml_files.txt

          # find ./data/bibl/tei -name '*.xml' | sort > all_files.txt
          # head -n 5000 all_files.txt > xml_files.txt
          # head -n 10000 all_files.txt | tail -n 5000 > xml_files.txt
          # head -n 15000 all_files.txt | tail -n 5000 > xml_files.txt
          # head -n 20000 all_files.txt | tail -n 5000 > xml_files.txt
          # head -n 25000 all_files.txt | tail -n 5000 > xml_files.txt
          # head -n 27000 all_files.txt | tail -n 5000 > xml_files_batch_01.txt

          # find ./data/taxonomy -name '*.rdf' > xml_files.txt

          # test cases
          # echo "./data/places/tei/5622.xml" > xml_files.txt
          # find ./data/works/tei -name '*.xml' | head -n 5 > xml_files.txt
          find ./data/bibl/tei -name '*.xml' | head -n 5 > xml_files.txt
          
          echo "Processing $(wc -l < xml_files.txt) XML files"
          
      # # Step 4: Run XSLT Transformations in Parallel for JSON: FOR OPENSEARCH INDEX UPDATES
      # - name: Run XSLT Transformations and Create Bulk JSON (Parallelized)
      #   run: |
      #     if [ ! -s xml_files.txt ]; then
      #       echo "No XML files to process."
      #       exit 0
      #     fi
      
      #     set +e  # Allow errors without stopping execution
      #     mkdir -p logs json_output  # Create log and JSON directories
      #     > bulk_data.json  # Clear existing bulk JSON file
      
      #     # Process each XML file in parallel
      #     cat xml_files.txt | xargs -P $(nproc) -I {} sh -c '
      #       file="$1"
      #       filename=$(basename "${file%.xml}")
      #       type=$(echo "$file" | grep -o -E "taxonomy|work|subject|person|place|bibl" | tail -n 1)
      
      #       # Fix bible/subject conflict using POSIX-compliant `test`
      #       if [ "$type" = "subject" ]; then
      #         type="subject"
      #       elif [ "$type" = "bibl" ]; then
      #         type="cbss"
      #       fi
      
      #       echo "Processing $filename for $type JSON"
      
      #       # Write index metadata to a separate temp file
      #       echo "{\"index\":{\"_index\":\"syriaca-index-12\",\"_id\":\"$type-$filename\"}}" > json_output/$filename.json
      
      #       # Run transformation, ensuring single-line output
      #       if ! java -jar saxon.jar -s:"$file" -xsl:json-stylesheet.xsl docType="$type" | tr -d "\n" >> json_output/$filename.json; then
      #         echo "::warning:: JSON transformation failed for $file"
      #         echo "Skipping $type $file due to transformation error." >> logs/errors.log
      #       fi
      
      #       echo "" >> json_output/$filename.json  # Ensure newline between entries
      #     ' sh {}
      
      #     # Merge all JSON files into a single bulk file
      #     cat json_output/*.json > bulk_data.json
      
      #     set -e  # Re-enable stopping on error

      # # Step 5: Configure AWS credentials
      # - name: Configure AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v2
      #   with:
      #     role-to-assume: ${{ secrets.AWS_SROPHE_ROLE }}
      #     aws-region: us-east-1
      #     role-session-name: GitHub-OIDC-data    

      # # Step 6: Upload JSON file to S3 OPENSEARCH UPDATE
      # - name: Upload JSON to S3
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
      #   run: |
      #     TIMESTAMP=$(date +%Y%m%d%H%M%S)
      #     aws s3 cp bulk_data.json s3://srophe-syriaca-front-end/json-data/advancedsearchfields/index_12_$TIMESTAMP.json

      # Step 7: Convert XML to HTML in Parallel For Website pages
      - name: Convert XML to HTML in Parallel (with syriacadata path)
        run: |
          mkdir -p logs
      
          cat xml_files.txt | xargs -P $(nproc) -I {} sh -c '
            file="$1"
            filename=$(basename "${file%.xml}")
      
            # Get full relative path inside data (e.g., johnofephesus/places/5577)
            # by trimming ./data/{type}/tei/
            type=$(echo "$file" | grep -o -E "taxonomy|work|subject|person|place|bibl" | tail -n 1)
            relative=$(echo "$file" | sed -E "s|^\.\/data\/${type}\/tei\/||" | sed "s/\.xml$//")
            echo "Relative: ${relative}"
            [ "$type" = "bibl" ] && mapped_type="cbss" || mapped_type="$type"
            [ "$type" = "subject" ] && mapped_type="taxonomy" || true
      
            # Final output path
            out_dir="data-html/${type}/syriacadata/$(dirname "$relative")"
            mkdir -p "$out_dir"
      
            echo "Converting $file â†’ $out_dir/$filename.html"
            if ! java -jar saxon.jar -s:"$file" -xsl:html-stylesheet.xsl -o:"$out_dir/$filename.html" 2>> logs/errors.log; then
              echo "::warning:: HTML transformation failed for $file"
              echo "Error processing $file" >> logs/errors.log
            fi
          ' sh {}
          
      # Step 8: Upload HTML files to S3 
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_SROPHE_ROLE }}
          aws-region: us-east-1
          role-session-name: GitHub-OIDC-data
          role-duration-seconds: 14400
          
      
      - name: Upload HTML files to S3 in parallel(Strip before syriacadata, Map Prefixes, Strip .html)
        run: |
          mkdir -p logs
          > logs/uploaded_files.txt
      
          find ./data-html -path "*/syriacadata/*.html" -print0 | xargs -0 -P $(nproc) -I {} sh -c '
            html_file="$1"
            echo "html_file: ${html_file}"
            # Extract the part of the path after "syriacadata/"
            s3_key=$(echo "$html_file" | sed -E "s|^.*?/syriacadata/||" | sed "s/\.html$//")
            echo "s3_key: ${s3_key}"
            # Map top-level dir if needed
            top_dir=$(echo "$s3_key" | cut -d/ -f1)
            case "$top_dir" in
              bibl) mapped_dir="cbss" ;;
              subject) mapped_dir="taxonomy" ;;
              *) mapped_dir="$top_dir" ;;
            esac
      
            rest_of_path=$(echo "$s3_key" | cut -d/ -f2-)
            if [ "$rest_of_path" = "$s3_key" ]; then
              new_key="$mapped_dir"
            else
              new_key="${mapped_dir}/${rest_of_path}"
            fi

            # Get the filename (last segment, like "2")
            filename=$(basename "$s3_key")
            echo "filename: ${filename}"
            # Get the type (first segment after johnofephesus/ if present, or first overall)
            if echo "$s3_key" | grep -q "^johnofephesus/"; then
              type=$(echo "$html_file" | sed -E "s|^.*data-html/([^/]+)/syriacadata/.*|\1|")
              type="${type}s"
            elif echo "$s3_key" | grep -q "^cbss/"; then
              type="bibl"
            else
              type=$(echo "$s3_key" | cut -d/ -f1)
              type="${type}s"
            fi

            # Determine path to original XML (always from canonical path)
            xml_path="./data/${type}/tei/${filename}.xml"
            echo "xml_path: ${xml_path}"
            s3_path="s3://srophe-syriaca-front-end/${new_key}"
            tei_s3_path="s3://srophe-syriaca-front-end/${new_key}.xml"

            echo "Uploading $html_file to $s3_path"
            aws s3 cp "$html_file" "$s3_path" && echo "$s3_path" >> logs/uploaded_files.txt
            echo "Uploading $xml_file to $tei_s3_path"
            aws s3 cp "$xml_path" "$tei_s3_path" && echo "$tei_s3_path" >> logs/uploaded_files.txt
          ' sh {}
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          AWS_REGION: ${{ secrets.AWS_REGION }}


      # Step 9: Upload error logs as an artifact for debugging
      - name: Upload Logs to GitHub Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: error-logs
          path: logs/errors.log
